{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adfdemohw"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/StagingParquet')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "sinkBlob",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "file.parquet",
						"folderPath": "dataflow",
						"container": "stagingdataflow"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "ProductCategoryID",
						"type": "INT32"
					},
					{
						"name": "ParentProductCategoryID",
						"type": "INT32"
					},
					{
						"name": "Name",
						"type": "UTF8"
					},
					{
						"name": "rowguid",
						"type": "UTF8"
					},
					{
						"name": "ModifiedDate",
						"type": "INT96"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/transactionparquet')]",
			"type": "Microsoft.DataFactory/factories/datasets",
			"apiVersion": "2018-06-01",
			"properties": {
				"linkedServiceName": {
					"referenceName": "adls2synapse",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "edlmtest"
					},
					"compressionCodec": "snappy"
				},
				"schema": [
					{
						"name": "Customer_Id",
						"type": "INT64"
					},
					{
						"name": "Record_Date",
						"type": "UTF8"
					},
					{
						"name": "Action",
						"type": "UTF8"
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/CopyPipeline_ctcdemo')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy_bts",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "SalesLT.SalesOrderHeader"
							},
							{
								"name": "Destination",
								"value": "stagingdataflow/sales/"
							}
						],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"partitionOption": "None"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "SourceDataset_bts",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_bts",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Transformation with Azure Databricks')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Run a simple Transformation job using Azure Databricks, with a single pane of glass monitoring from ADF.\n\nIn the template, we check for source dataset availability. Once it is available we copy it into a blob storage for staging using a Copy activity. The same storage is accessed from Databricks clusters while processing the data (Transformation). The output is stored in the same storage under 'output' folder. Various notebook properties are referenced as expressions using pipeline parameters, which lets you configure more generic and reusable pipelines.",
				"activities": [
					{
						"name": "Availability flag",
						"description": "Validation activity is used to get information about the source files if they are available for processing. \nIn this template, only when the '_success' flag/ file is available at source, would the downstream activities be triggered. ",
						"type": "Validation",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "SourceAvailabilityDataset",
								"type": "DatasetReference",
								"parameters": {}
							},
							"timeout": "7.00:00:00",
							"sleep": 10
						}
					},
					{
						"name": "file-to-blob",
						"description": "Copy activity copies the actual files/ dataset to be processed by Databricks into a staging store. This storage should be accessible by the Azure Databricks cluster referenced in the next activity. ",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Availability flag",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"dataIntegrationUnits": 0
						},
						"inputs": [
							{
								"referenceName": "SourceFilesDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationFilesDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Transformation",
						"description": "Databricks Notebook activity does the processing of the data copied in the previous step (copy activity).  Please ensure you have added the databricks notebook (<a href='https://adflabstaging1.blob.core.windows.net/share/Transformations.html' target='_blank'>https://adflabstaging1.blob.core.windows.net/share/Transformations.html</a>) in the databricks work-space and referenced it in the notebook activity in ADF.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "file-to-blob",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/adftutorial/Transformations",
							"baseParameters": {
								"input": {
									"value": "@pipeline().parameters.inputPath",
									"type": "Expression"
								},
								"output": {
									"value": "@pipeline().parameters.outputPath",
									"type": "Expression"
								},
								"filename": {
									"value": "@pipeline().parameters.fileName",
									"type": "Expression"
								},
								"pipelineRunId": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "AzureDatabricksLab",
							"type": "LinkedServiceReference"
						}
					}
				],
				"parameters": {
					"inputPath": {
						"type": "String",
						"defaultValue": "/staged_sink"
					},
					"outputPath": {
						"type": "String",
						"defaultValue": "/processed_sink"
					},
					"fileName": {
						"type": "String",
						"defaultValue": "Product.csv"
					}
				},
				"annotations": [],
				"lastPublishTime": "2020-12-07T19:22:15Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Transformation')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "StagingParquet",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "SelectedColumns",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "Select1"
						}
					],
					"script": "source(output(\n\t\tProductCategoryID as integer,\n\t\tParentProductCategoryID as integer,\n\t\tName as string,\n\t\trowguid as string,\n\t\tModifiedDate as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tignoreNoFilesFound: false,\n\tformat: 'parquet') ~> source1\nsource1 select(mapColumn(\n\t\tProductCategoryID,\n\t\tName,\n\t\tModifiedDate\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> Select1\nSelect1 sink(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> sink1"
				}
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/StagingParquet')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/lookup')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Lookup1",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "ParquetSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFileName": "*.parquet",
									"enablePartitionDiscovery": false
								}
							},
							"dataset": {
								"referenceName": "transactionparquet",
								"type": "DatasetReference",
								"parameters": {}
							},
							"firstRowOnly": false
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/transactionparquet')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/DataflowCopy')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Run dataflow with Explicit copy to ingest copy source.",
				"activities": [
					{
						"name": "LoadData",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlSource",
								"partitionOption": "None"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "ProductCategoryID"
										},
										"sink": {
											"name": "ProductCategoryID"
										}
									},
									{
										"source": {
											"name": "ParentProductCategoryID"
										},
										"sink": {
											"name": "ParentProductCategoryID"
										}
									},
									{
										"source": {
											"name": "Name"
										},
										"sink": {
											"name": "Name"
										}
									},
									{
										"source": {
											"name": "rowguid"
										},
										"sink": {
											"name": "rowguid"
										}
									},
									{
										"source": {
											"name": "ModifiedDate"
										},
										"sink": {
											"name": "ModifiedDate"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "AzureSqlDatabaseSourceDatasetEntity",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "StagingParquet",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Transformation",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "LoadData",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Transformation",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"annotations": [],
				"lastPublishTime": "2020-12-07T19:48:33Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/datasets/StagingParquet')]",
				"[concat(variables('factoryId'), '/dataflows/Transformation')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/Master Pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "DataflowCopy",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "DataflowCopy",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Transformation with Databricks",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "DataflowCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Transformation with Azure Databricks",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "AML",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Transformation with Databricks",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "AML",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/pipelines/DataflowCopy')]",
				"[concat(variables('factoryId'), '/pipelines/Transformation with Azure Databricks')]"
			]
		}
	]
}